# ğŸ¬ ReelSense

## Explainable Movie Recommender System with Diversity Optimization

> **BrainDead @ Revelation 2K26 â€“ Problem Statement 1**
> Department of Computer Science and Technology, IIEST Shibpur

---

## ğŸ“Œ Problem Statement Alignment (BrainDead â€“ PS1)

ReelSense directly addresses **Problem Statement 1** of the BrainDead competition. The objective is to design a **Topâ€‘K Movie Recommendation System** that:

* Generates **personalized recommendations** using hybrid approaches
* Ensures **diversity and catalog coverage** to mitigate popularity bias
* Provides **naturalâ€‘language explanations** for each recommendation
* Reports **ranking, diversity, and novelty metrics**

This repository contains the **complete pipeline**, from data preprocessing and EDA to model training, evaluation, and explainability.

---

## ğŸ“‚ Dataset Description

**Dataset:** MovieLens Latest Small
**Source:** GroupLens Research
**License:** MovieLens Terms of Use

**Dataset Size**

* 100,836 ratings
* 610 users
* 9,742 movies

**Files Used**

* `ratings.csv` â€“ User ratings (0.5 to 5.0)
* `movies.csv` â€“ Movie metadata (title, genres)
* `tags.csv` â€“ Userâ€‘assigned freeâ€‘form tags
* `links.csv` â€“ External identifiers (IMDB, TMDb)

---

## ğŸ§¹ Preprocessing Pipeline

The following preprocessing steps were applied:

* Timeâ€‘based **trainâ€“test split** (leaveâ€‘lastâ€‘N ratings per user)
* Removal of noisy and duplicate tags
* Standardization of genre and tag tokens
* Construction of **userâ€“item interaction matrix**
* Timestamp parsing and conversion to datetime
* Normalization of popularity statistics for diversity metrics

---

## ğŸ” Exploratory Data Analysis (EDA)

Key analyses performed:

* Rating distribution analysis
* Genre popularity vs. average rating
* User activity histogram (ratings per user)
* Longâ€‘tail distribution of movies
* Temporal trends in rating behavior

EDA visualizations are included in the `/notebooks` directory and are clearly labeled for interpretation.

---

## ğŸ§  Models Implemented

### 1ï¸âƒ£ Popularityâ€‘Based Baseline

* Topâ€‘N mostâ€‘rated movies
* Topâ€‘N highestâ€‘averageâ€‘rated movies

### 2ï¸âƒ£ Collaborative Filtering

* Userâ€“User Collaborative Filtering
* Itemâ€“Item Collaborative Filtering

### 3ï¸âƒ£ Matrix Factorization

* Singular Value Decomposition (SVD)
* Implemented using the **Surprise** library
* Optimized for RMSE minimization

### 4ï¸âƒ£ Hybrid Recommendation Model

A weighted hybrid of:

* Collaborative scores (SVD)
* Content similarity (genres + tags)
* Bayesianâ€‘smoothed global ratings

---

## ğŸ§  Hybrid Scoring Logic

The final recommendation score is computed as:

$$
Score_{final} = (\alpha \cdot P_{SVD}) + (\beta \cdot P_{Content}) - (\gamma \cdot Popularity_{norm})
$$

Where:

* $P_{SVD}$ captures latent user preferences
* $P_{Content}$ captures semantic similarity
* $Popularity_{norm}$ penalizes overâ€‘popular items

This formulation improves **novelty and catalog coverage** while retaining accuracy.

---

## âœ¨ Explainability Layer

Each recommended movie is accompanied by a **humanâ€‘readable explanation**, generated using the dominant contributing signal.

**Explanation Sources**

* Tag similarity
* Genre overlap
* Collaborative neighborhood similarity
* Global consensus (critically acclaimed but underâ€‘watched)

**Example Explanation**

> "Because you liked *Inception* and *The Matrix*, which share the tags *sciâ€‘fi* and *mindâ€‘bending*."

---

## ğŸŒ Diversity & Novelty Strategy

To mitigate popularity bias ("Harry Potter Effect"), ReelSense applies:

* Popularityâ€‘normalized penalties
* Reâ€‘ranking for longâ€‘tail exposure
* Intraâ€‘list diversity optimization

This ensures recommendations are **accurate, diverse, and nonâ€‘repetitive**.

---

## ğŸ“Š Evaluation Metrics (As per BrainDead Guidelines)

### A. Rating Prediction

* RMSE
* MAE

### B. Topâ€‘K Recommendation (K = 10)

* Precision@10
* Recall@10
* NDCG@10
* MAP@10

### C. Diversity & Novelty

* Catalog Coverage
* Intraâ€‘List Diversity
* Popularityâ€‘Normalized Hits

Baseline models are used for comparative benchmarking.

---

## ğŸ—ï¸ System Architecture

The ReelSense system is designed with a clear separation between **offline model training** and **online inference**, ensuring scalability, interpretability, and low-latency recommendations.

````mermaid
graph LR

    %% User Interaction Layer
    U[User] -->|Preferences & History| FE[Frontend UI]
    FE -->|REST Request| API[FastAPI Backend]

    %% Online Inference Pipeline
    subgraph Online Inference
        API --> PROF[User Profile Builder]
        PROF --> AGG[Hybrid Score Aggregator]
        AGG --> RERANK[Diversity Re-Ranker]
        RERANK --> EXPL[Explainability Engine]
    end

    %% Offline Training Pipeline
    subgraph Offline Training
        DATA[(MovieLens Dataset)] --> PRE[Preprocessing & EDA]
        PRE --> SVD[SVD Collaborative Model]
        PRE --> CONT[Content Model (Genres + Tags)]
        PRE --> BAY[Bayesian Popularity Smoothing]
    end

    %% Model Integration
    SVD --> AGG
    CONT --> AGG
    BAY --> AGG

    %% Final Response
    EXPL -->|Top-K Movies + Explanations| API
    API --> FE
```mermaid
graph LR

    %% User Interaction Layer
    U[User] -->|Preferences / History| FE[Frontend UI]
    FE -->|API Request| API[FastAPI Service]

    %% Online Inference Pipeline
    subgraph Online Inference
        API --> PROF[User Profile Builder]
        PROF --> AGG[Hybrid Score Aggregator]
        AGG --> RERANK[Diversity Re-Ranker]
        RERANK --> EXPL[Explainability Engine]
    end

    %% Offline Training Pipeline
    subgraph Offline Training
        DATA[(MovieLens Dataset)] --> PRE[Preprocessing & EDA]
        PRE --> CF[Collaborative Model (SVD)]
        PRE --> CB[Content Model (Genres + Tags)]
        PRE --> POP[Popularity & Bayesian Smoothing]
    end

    %% Model Usage
    CF --> AGG
    CB --> AGG
    POP --> AGG

    %% Response
    EXPL -->|Top-K Movies + Explanations| API
    API --> FE
````

---

## ğŸ“¦ Project Structure

```
ReelSense/
â”œâ”€â”€ data/                 # Data loading & preprocessing scripts
â”œâ”€â”€ notebooks/            # EDA & experiment notebooks
â”œâ”€â”€ recommender/          # Recommendation models
â”œâ”€â”€ explainability/       # Explanation generation logic
â”œâ”€â”€ evaluation/           # Metric computation
â”œâ”€â”€ app/                  # FastAPI backend
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“¦ Deliverables Checklist (BrainDead)

* âœ”ï¸ Concise technical report (PDF / PPT)
* âœ”ï¸ Public GitHub repository
* âœ”ï¸ iPython notebooks for EDA & modeling
* âœ”ï¸ Model training and evaluation code
* âœ”ï¸ Explainability demonstrations

---

## ğŸ‘¥ Team Details

**Department of Computer Science and Technology**
**IIEST Shibpur**

* **Rachit** â€“ Lead ML Engineer & System Architect
* **Sarvesh** â€“ Frontend Developer & UI/UX
* **Atharva** â€“ Data Analyst & Evaluation Specialist

---

## â¤ï¸ Acknowledgements

Built with passion for **Revelation 2K26 â€“ BrainDead**.
May our models generalize, our bias reduce, and our recommendations make sense.


